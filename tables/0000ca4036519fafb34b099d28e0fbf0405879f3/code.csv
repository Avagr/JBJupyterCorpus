cell_id,execution_count,source
0,1.0,"['#setup\n', ""data_dir='../../Data/Weather'\n"", ""file_index='SSSBSSSB'\n"", ""meas='PRCP'""]"
3,2.0,"['# Enable automiatic reload of libraries\n', '#%load_ext autoreload\n', '#%autoreload 2 # means that all modules are reloaded before every command']"
4,3.0,"['%pylab inline\n', 'import numpy as np\n', '\n', 'import findspark\n', 'findspark.init()\n', '\n', 'import sys\n', ""sys.path.append('./lib')\n"", '\n', 'from numpy_pack import packArray,unpackArray\n', '\n', 'from Eigen_decomp import Eigen_decomp\n', 'from YearPlotter import YearPlotter\n', 'from recon_plot import recon_plot\n', '\n', 'from import_modules import import_modules,modules\n', 'import_modules(modules)\n', '\n', 'from ipywidgets import interactive,widgets']"
5,4.0,"['from pyspark import SparkContext\n', '#sc.stop()\n', '\n', 'sc = SparkContext(master=""local[3]"",pyFiles=[\'lib/numpy_pack.py\',\'lib/spark_PCA.py\',\'lib/computeStats.py\',\'lib/recon_plot.py\',\'lib/Eigen_decomp.py\'])\n', '\n', 'from pyspark import SparkContext\n', 'from pyspark.sql import *\n', 'sqlContext = SQLContext(sc)']"
7,5.0,"['from pickle import load\n', '\n', '#read statistics\n', ""filename=data_dir+'/STAT_%s.pickle'%file_index\n"", ""STAT,STAT_Descriptions = load(open(filename,'rb'))\n"", 'measurements=STAT.keys()\n', ""print 'keys from STAT=',measurements""]"
9,6.0,"['#read data\n', ""filename=data_dir+'/decon_%s_%s.parquet'%(file_index,meas)\n"", 'df_in=sqlContext.read.parquet(filename)\n', '#filter in \n', 'df=df_in.filter(df_in.measurement==meas)\n', 'df.show(5)']"
11,7.0,"['m=meas\n', ""fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,6));\n"", 'k=3\n', ""EigVec=np.matrix(STAT[m]['eigvec'][:,:k])\n"", ""Mean=STAT[m]['Mean']\n"", ""YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n"", ""YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])""]"
13,8.0,"['#  x=0 in the graphs below correspond to the fraction of the variance explained by the mean alone\n', '#  x=1,2,3,... are the residuals for eig1, eig1+eig2, eig1+eig2+eig3 ...\n', 'fig,ax=plt.subplots(1,1);\n', ""eigvals=STAT[m]['eigval']; eigvals/=sum(eigvals); cumvar=cumsum(eigvals); cumvar=100*np.insert(cumvar,0,0)\n"", 'ax.plot(cumvar[:10]); \n', 'ax.grid(); \n', ""ax.set_ylabel('Percent of variance explained')\n"", ""ax.set_xlabel('number of eigenvectors')\n"", ""ax.set_title('Percent of variance explained');""]"
15,9.0,"['# A function for plotting the CDF of a given feature\n', 'def plot_CDF(df,feat):\n', '    rows=df.select(feat).sort(feat).collect()\n', '    vals=[r[feat] for r in rows]\n', '    P=np.arange(0,1,1./(len(vals)))\n', '    while len(vals)< len(P):\n', '        vals=[vals[0]]+vals\n', '    plot(vals,P)\n', ""    title('cumulative distribution of '+feat)\n"", ""    ylabel('fraction of instances')\n"", '    xlabel(feat)\n', '    grid()\n', '    ']"
16,10.0,"[""plot_CDF(df,'res_3')""]"
17,11.0,"[""rows=df.rdd.map(lambda row:(row.station,row.year,unpackArray(row['vector'],np.float16))).collect()\n"", 'rows[0][:2]']"
18,12.0,"['days=set([r[1] for r in rows])\n', 'miny=min(days)\n', 'maxy=max(days)\n', 'record_len=int((maxy-miny+1)*365)\n', 'record_len']"
19,13.0,"['## combine the measurements for each station into a single long array with an entry for each day of each day\n', 'All={}  # a dictionary with a numpy array for each day of each day\n', 'i=0\n', 'for station,day,vector in rows:\n', '    i+=1; \n', '    # if i%1000==0: print i,len(All)\n', '    if not station in All:\n', '        a=np.zeros(record_len)\n', '        a.fill(np.nan)\n', '        All[station]=a\n', '    loc = int((day-miny)*365)\n', '    All[station][loc:loc+365]=vector']"
20,14.0,"['from datetime import date\n', 'd=datetime.date(int(miny), month=1, day=1)\n', 'start=d.toordinal()\n', 'dates=[date.fromordinal(i) for i in range(start,start+record_len)]']"
21,15.0,"['for station in All:\n', '    print station, np.count_nonzero(~np.isnan(All[station]))']"
22,16.0,"['Stations=sorted(All.keys())\n', 'A=[]\n', 'for station in Stations:\n', '    A.append(All[station])\n', '\n', 'day_station_table=np.hstack([A])\n', 'print shape(day_station_table)']"
23,17.0,"['def RMS(Mat):\n', '    return np.sqrt(np.nanmean(Mat**2))\n', '\n', 'mean_by_day=np.nanmean(day_station_table,axis=0)\n', 'mean_by_station=np.nanmean(day_station_table,axis=1)\n', 'tbl_minus_day = day_station_table-mean_by_day\n', 'tbl_minus_station = (day_station_table.transpose()-mean_by_station).transpose()\n', '\n', ""print 'total RMS                   = ',RMS(day_station_table)\n"", ""print 'RMS removing mean-by-station= ',RMS(tbl_minus_station)\n"", ""print 'RMS removing mean-by-day   = ',RMS(tbl_minus_day)""]"
24,18.0,"['RT=day_station_table\n', 'F=RT.flatten()\n', 'NN=F[~np.isnan(F)]\n', '\n', 'NN.sort()\n', 'P=np.arange(0.,1.,1./len(NN))\n', 'plot(NN,P)\n', 'grid()\n', ""title('CDF of daily rainfall')\n"", ""xlabel('daily rainfall')\n"", ""ylabel('cumulative probability')""]"
27,19.0,"['from scipy.special import gammaln,factorial\n', '#for i in range(10):\n', '#    print exp(gammaln(i+1))-factorial(i)\n', 'def G(n):\n', '    return gammaln(n+1)\n', 'def LogProb(m,l,n1,n2):\n', '    logP=-G(l)-G(n1-l)-G(n2-l)-G(m-n1-n2+l)-G(m)+G(n1)+G(m-n1)+G(n2)+G(m-n2)\n', '    return logP/m\n', 'exp(LogProb(1000,0,500,500))']"
28,20.0,"['#USC00193270 21482\n', '#USC00193702 28237\n', ""X=copy(All['USC00041281'])\n"", ""Y=copy(All['USW00093227'])\n"", 'print sum(~np.isnan(X))\n', 'print sum(~np.isnan(Y))\n', 'X[np.isnan(Y)]=np.nan\n', 'Y[np.isnan(X)]=np.nan\n', 'print sum(~np.isnan(X))\n', 'print sum(~np.isnan(Y))\n']"
29,21.0,"['def computeLogProb(X,Y):\n', '    X[np.isnan(Y)]=np.nan\n', '    Y[np.isnan(X)]=np.nan\n', '    G=~isnan(X)\n', '    m=sum(G)\n', '    XG=X[G]>0\n', '    YG=Y[G]>0\n', '    n1=sum(XG)\n', '    n2=sum(YG)\n', '    l=sum(XG*YG)\n', '    logprob=LogProb(m,l,n1,n2)\n', ""    # print 'm=%d,l=%d,n1=%d,n2=%d,LogPval=%f'%(m,l,n1,n2,logprob)\n"", '    return logprob,m\n', 'print computeLogProb(X,Y)']"
31,22.0,"['L=len(Stations)\n', 'Pvals=np.zeros([L,L])\n', 'Length=np.zeros([L,L])\n', 'P_norm=np.zeros([L,L])\n', 'for i in range(L):\n', '    print i,\n', '    for j in range(L):\n', '        if i==j: \n', '            P_norm[i,j]=-0.4\n', '            continue\n', '        X=copy(All[Stations[i]])\n', '        Y=copy(All[Stations[j]])\n', '        P_norm[i,j],Length[i,j]=computeLogProb(X,Y)\n', '        if Length[i,j]<200:\n', '            P_norm[i,j]=np.nan\n', '\n', '            ']"
32,23.0,"['print Pvals[:2,:2]\n', 'print Length[:2,:2]\n', 'print P_norm[:2,:2]']"
33,24.0,"['A=P_norm.flatten();\n', 'B=A[~isnan(A)]\n', 'print A.shape,B.shape\n', 'hist(-B,bins=100);\n', ""xlabel('significance')""]"
34,25.0,"['def showmat(mat):\n', '    fig,axes=plt.subplots(1,1,figsize=(10,10))\n', '    axes.imshow(mat, cmap=plt.cm.gray)']"
35,26.0,['showmat(P_norm)']
37,27.0,"[""print 'A group of very correlated stations is:',All.keys()[:8]""]"
38,28.0,"['from sklearn.decomposition import PCA\n', 'P_norm0 = np.nan_to_num(P_norm)\n', 'n_comp=10\n', ""pca = PCA(n_components=n_comp, svd_solver='full')\n"", 'pca.fit(P_norm0)     \n', '#print(pca.explained_variance_)\n', 'Var_explained=pca.explained_variance_ratio_\n', 'plot(insert(cumsum(Var_explained),0,0))\n', 'grid()']"
39,29.0,"['# we will look only at the top 4 eigenvectors.\n', 'n_comp=4\n', ""pca = PCA(n_components=n_comp, svd_solver='full')\n"", 'pca.fit(P_norm0)     ']"
40,30.0,"[""fig,axes=plt.subplots(1,4,figsize=(20,5),sharey='row');\n"", 'L=list(pca.components_.transpose())\n', 'for i in range(4):\n', '    X=sorted(L,key=lambda x:x[i]) \n', '    axes[i].plot(X);']"
41,31.0,"['def re_order_matrix(M,order):\n', '    M_reord=M[order,:]\n', '    M_reord=M_reord[:,order]\n', '    return M_reord']"
42,32.0,"[""fig,axes=plt.subplots(2,2,figsize=(15,15),sharex='col',sharey='row');\n"", 'i=0\n', 'for r in range(2):\n', '    for c in range(2):\n', '        order=np.argsort(pca.components_[i,:])\n', '        P_norm_reord=re_order_matrix(P_norm0,order)\n', '        axes[r,c].matshow(P_norm_reord)\n', '        i+=1']"
44,33.0,"['from pickle import dump\n', ""with open(data_dir+'/PRCP_residuals_PCA.pickle','wb') as file:\n"", ""    dump({'stations':All.keys(),\n"", ""          'eigen-vecs':pca.components_},\n"", '        file)\n', '    ']"
45,,[]
