cell_id,cell_type,source
0,code,"[""variable= 'pct_ch_hinc00_16'\n"", ""binary6typ = variable+'_binary'\n"", ""model_name = '4_CENSUS_FQ_'+variable+'NYC'""]"
1,markdown,"['# Analysis of Foursquare Data\n', '# NYU - CUSP UDP Capstone\n', '# Foursquare + NYC Merge by CT \n', '## Brief\n', 'This notebook \n', '1. Part I. Data Processing\n', '\n', '    - import Foursquare data\n', '    - imports the Census Tract shapefile\n', '    - import Typologies\n', '    - merges topologies\n', '    - Spatail join by Census Tract\n', '    - merges topologies\n', '    - map topologies\n', '    - map Businesses\n', '1. Part II. Data Processing\n', '    - Performs a classifcation task on Typologies\n', '###  You can refer to https://github.com/mv1742/updny_2']"
2,markdown,['# Part I. Data Processing\n']
3,code,"['import os\n', 'import requests\n', 'import shapely\n', 'import json\n', '\n', 'import matplotlib.pyplot as plt #plotting\n', 'import pandas as pd\n', 'import numpy as np\n', 'import scipy.stats as stat\n', '#make sure plots are embedded into the notebook\n', '%matplotlib inline\n', '#import statsmodels.formula.api as smf\n', 'import itertools\n', 'import geopandas as gpd\n', 'from shapely.geometry import Point, Polygon\n', 'from sklearn.metrics import silhouette_score\n', '# from sklearn.mixture import GaussianMixture\n', 'from scipy import linalg\n', 'import matplotlib as mpl\n', 'import pandas as pd\n', 'from sklearn import preprocessing\n', 'from scipy.cluster.hierarchy import dendrogram, linkage  # for hierarchical clustering\n', 'from scipy.cluster.hierarchy import fcluster\n', '# from sklearn.mixture import GaussianMixture\n', 'from scipy import linalg\n', 'import matplotlib as mpl\n', 'import pandas as pd\n', 'from sklearn import preprocessing\n', 'from scipy.cluster.hierarchy import dendrogram, linkage  # for hierarchical clustering\n', 'from scipy.cluster.hierarchy import fcluster\n', 'import pandas as pd\n', 'import numpy as np\n', 'from matplotlib import pyplot as plt\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.cluster import KMeans\n', 'import geopandas as gpd\n', 'import pylab as pl\n', 'import io\n', 'import pylab as pl\n', '\n', 'from geopandas import GeoDataFrame\n', 'from geopandas.tools import sjoin\n', 'try:\n', '    import ipywidgets as widgets\n', '    hasWidgets = True\n', 'except ImportError:\n', '    hasWidgets = False\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn import svm\n', '# from sklearn.metrics import confusion_matrix\n', '# from sklearn import datasets\n', 'from sklearn.decomposition import PCA\n', 'from sklearn import preprocessing\n', 'from sklearn.metrics import roc_auc_score\n', 'from sklearn.ensemble import RandomForestClassifier\n', 'from sklearn.model_selection import GridSearchCV\n', 'from sklearn.tree import DecisionTreeClassifier\n', 'from sklearn.metrics import f1_score\n', 'from sklearn.metrics import recall_score\n', '# from sklearn.metrics import precision_score\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, classification_report\n', 'import matplotlib.pylab\n', 'import sklearn.preprocessing as preprocessing\n', 'from sklearn.preprocessing import MinMaxScaler\n', 'from sklearn.model_selection import train_test_split\n', '\n', '%pylab inline']"
4,markdown,"['# NYU - CUSP UDP Capstone\n', '# Foursquare + NYC Merge by CT \n', '## Brief\n', 'This notebook \n', '- import Foursquare data\n', '- imports the Census Tract shapefile\n', '- import Typologies\n', '- merges topologies\n', '- Spatail join by Census Tract\n', '- merges topologies\n', '- map topologies\n', '- map Businesses\n', '- outputs a .csv of the results\n', '- You can refer to https://github.com/mv1742/updny_2']"
5,code,"[""# # url = 'https://planninglabs.carto.com/api/v2/sql?filename=region&q=SELECT%20%2A%20FROM%20region_censustract_v0&format=SHP'\n"", '# # NYCzip = getGeoDataFrameFromShpFileZipUrl(url)\n', '\n', '# # GREATER NY REGION\n', ""# NYCzip=gpd.read_file('Censustracts/region.shp')\n"", '# NYCzip.plot()\n']"
6,code,"['# NYC ONLY\n', ""NYCzip=gpd.read_file('./Data/NYC/14000.shp')\n"", 'NYCzip.rename(columns={""GEO_ID"": ""GEOID""},inplace=True)\n', 'NYCzip.plot()\n', 'print(NYCzip.columns,len(NYCzip))\n', 'NYCzip.GEOID = NYCzip.GEOID.str[-11:]\n', '\n', 'NYCzip.head(2)']"
7,code,['NYCzip.columns']
8,code,"['NYCzip.rename(columns={""geoid"": ""GEOID""},inplace=True)\n', 'NYCzip.GEOID = NYCzip.GEOID.astype(int)\n', ""cols = ['GEOID','geometry']\n"", 'NYCzip = NYCzip.loc[:,cols]\n', ""#NYCzipgdp.plot(column='GEOID',legend = True)\n"", 'NYCzip.shape\n', 'NYCzip.isnull().sum()']"
9,code,"['figure, ax = plt.subplots(figsize=(5, 5))\n', ""NYCzip.plot(column='GEOID',legend = True, ax=ax)""]"
10,markdown,"['# Typologies\n', ""- import raw Typology file with Census data 'NY_final_data_for_typologies_1.19.19.csv'\n"", '- merges Typologies with the new Binary typologies\n', '- map topologies']"
11,code,"[""Typologiespd=pd.read_csv('NY_final_data_for_typologies_1.19.19.csv')\n"", 'Typologiesgdp = gpd.GeoDataFrame(Typologiespd)\n', 'len(Typologiesgdp.columns)']"
12,code,"['Typologiesgdp.rename(columns={\'geoid\': ""GEOID""},inplace=True)\n', 'Typologiesgdp.tail()\n', ""cols_typ = ['GEOID','Type_1.19']\n"", 'print(type(Typologiesgdp.iloc[:,0][0]))\n', '#Typologies.rename(columns={\'\\ufeffgeoid\': ""GEOID""},inplace=True)\n', '#Typologiesgdp.geoid = Typologies.iloc[:,0]\n', 'Typologiesgdp = Typologiesgdp.loc[:,cols_typ]\n', 'Typologiesgdp.head()']"
13,code,"[""Binaries=pd.read_csv('./Data/NEW_6_BINARIES_ALL.csv')\n"", 'len(Binaries.columns), Binaries.shape\n', ""Binaries.drop(columns = 'Unnamed: 0', inplace=True)\n"", 'Binaries.head()']"
14,code,"[""bin_typs = ['pct_ch_hinc00_16_binary',\n"", ""            'pct_ch_medhval00_16_binary','pct_ch_medrent00_16_binary','pct_ch_percol00_16_binary','Ongoing_adv_gent',\n"", ""            'gent00_16','gent90_00','Supergent16']\n"", 'for i, column in enumerate(bin_typs):\n', '    plt.figure(1)\n', '    plt.subplot(4,4,i+1)\n', ""    Binaries[column].value_counts().plot(kind='bar', figsize = (15,15), title=column)\n"", '    Binaries[column].value_counts()/Binaries[column].value_counts().sum()']"
15,code,"[""Typologiesgdp = Typologiesgdp.merge(Binaries, on= 'GEOID')\n"", 'Typologiesgdp.shape']"
16,code,['Typologiesgdp.isnull().sum()\n']
17,code,"[""merged = NYCzip.merge(Typologiesgdp,on='GEOID')\n"", 'merged.tail(1)']"
18,code,"['print(type(merged),merged.shape)\n', 'mergedgpd = gpd.GeoDataFrame(merged)\n', 'mergedgpd.shape']"
19,markdown,['# LOAD FOURSQUARE DATA']
20,code,"['# Neighbours\n', ""X_foursquare_neighbours = pd.read_csv('Improve_Features/X_foursquare-neighbours.csv')\n"", 'print(X_foursquare_neighbours.shape,X_foursquare_neighbours.columns)\n', ""X_foursquare_neighbours.drop(['Unnamed: 0'],axis =1, inplace=True)\n"", 'cols_neighbours = []\n', 'for column in X_foursquare_neighbours.columns:\n', ""    if '_sum10' in column:\n"", '        cols_neighbours.append(column)\n', ""cols_neighbours = cols_neighbours + ['GEOID']""]"
21,code,"['# Distances\n', ""X_foursquare_high = pd.read_csv('Improve_Features/X_foursquare-distances_high.csv')\n"", '\n', 'print(X_foursquare_high.shape,X_foursquare_high.columns)\n', 'cols_high = []\n', 'for column in X_foursquare_high.columns:\n', ""    if 'distance' in column:\n"", '        cols_high.append(column)\n', ""cols_high = cols_high + ['GEOID']\n"", 'cols_high']"
22,code,"[""X_foursquare_low = pd.read_csv('Improve_Features/X_foursquare-distances_low.csv')\n"", 'print(X_foursquare_low.shape,X_foursquare_low.columns)\n', 'cols_low = []\n', 'for column in X_foursquare_low.columns:\n', ""    if 'distance' in column:\n"", '        cols_low.append(column)\n', ""cols_low = cols_low + ['GEOID']\n"", 'cols_low']"
23,code,"['# Distances- numeric \n', ""X_foursquare_numeric = pd.read_csv('Improve_Features/X_foursquare-distances_numeric.csv')\n"", 'print(X_foursquare_numeric.shape,X_foursquare_numeric.columns)\n', 'cols_numeric = []\n', 'for column in X_foursquare_numeric.columns:\n', ""    if 'distance' in column:\n"", '        cols_numeric.append(column)\n', ""cols_numeric = cols_numeric + ['GEOID']\n"", 'cols_numeric\n', '# Distances- cats \n', ""X_foursquare_cats1 = pd.read_csv('Improve_Features/X_foursquare-distances_cats.csv')\n"", 'print(X_foursquare_cats1.shape,X_foursquare_cats1.columns)\n', 'cols_cats1 = []\n', 'for column in X_foursquare_cats1.columns:\n', ""    if 'distance' in column:\n"", '        cols_cats1.append(column)\n', ""cols_cats1 = cols_cats1 + ['GEOID']\n"", 'cols_cats1\n', '# Distances- class\n', ""X_foursquare_class = pd.read_csv('Improve_Features/X_foursquare-distances_class.csv')\n"", 'print(X_foursquare_class.shape,X_foursquare_cats1.columns)\n', 'cols_class = []\n', 'for column in X_foursquare_class.columns:\n', ""    if 'distance' in column:\n"", '        cols_class.append(column)\n', ""cols_class = cols_class + ['GEOID']\n"", 'cols_class']"
24,code,"['X_foursquare_neighbours = X_foursquare_neighbours.loc[:,cols_neighbours]\n', 'X_foursquare_high = X_foursquare_high.loc[:,cols_high]\n', 'X_foursquare_class = X_foursquare_class.loc[:,cols_class]\n', 'X_foursquare_cats1 = X_foursquare_cats1.loc[:,cols_cats1]\n', 'X_foursquare_numeric = X_foursquare_numeric.loc[:,cols_numeric]\n', 'X_foursquare_low = X_foursquare_low.loc[:,cols_low]\n', '# X_foursquare_person = X_foursquare_person.loc[:,cols_person]\n', '# X_foursquare_income = X_foursquare_income.loc[:,cols_income]']"
25,code,"['# X_foursquare_low.shape, X_foursquare_person.shape\n', 'print(X_foursquare_high.shape)']"
26,code,"[""X_foursquare_data_sjoined = pd.read_csv('./Data/data_sjoined_sum_merged.csv')\n"", ""X_foursquare_data_sjoined.drop(columns=['Unnamed: 0','Typologies'],inplace=True)\n"", 'print(X_foursquare_data_sjoined.shape)\n', 'print(X_foursquare_data_sjoined.shape,X_foursquare_data_sjoined.columns)\n', 'cols_datasjoined = []\n', ""for column in X_foursquare_data_sjoined.drop(columns=['geometry']).columns:\n"", ""    if 'Type' not in column:\n"", '        cols_datasjoined.append(column)\n', 'cols_datasjoined = cols_datasjoined\n', 'cols_datasjoined']"
27,code,"[""X_foursquare_data_sjoined_all = X_foursquare_data_sjoined.drop(columns=['geometry'])""]"
28,code,"[""# X_foursquare_data_sjoined_pca = X_foursquare_data_sjoined.drop(columns=['geometry']).iloc[:,7:698]\n"", '\n', 'X_foursquare_data_sjoined_non_type = X_foursquare_data_sjoined.loc[:,cols_datasjoined]\n', 'X_foursquare_data_sjoined_non_type.head(1)']"
29,code,"[""# X_foursquare_data_sjoined_other1 = X_foursquare_data_sjoined.drop(columns=['geometry']).iloc[:,698:]\n"", '# X_foursquare_data_sjoined_other1.head(1)']"
30,code,"[""# X_foursquare_data_sjoined_other2 = X_foursquare_data_sjoined.drop(columns=['geometry']).iloc[:,:7]\n"", '# X_foursquare_data_sjoined_other2.head(1)']"
31,markdown,['# LOAD TWITTER DATA']
32,code,"['# alltwitterandcensusdata.csv\n', ""alltwitterandcensusdata = pd.read_csv('Data/alltwitterandcensusdata.csv')\n"", 'alltwitterandcensusdata.rename(columns={\'geojoin\': ""GEOID""},inplace=True)\n', ""alltwitterandcensusdata.drop(columns='Unnamed: 0',inplace=True)\n"", 'print(alltwitterandcensusdata.shape)\n', ""twittercols = list(alltwitterandcensusdata.columns[:-23])+['distToHighVisitorTract',\\\n"", ""                                                     'distToHighTweetTract','distToMHI']\n"", ""print('twittercols------------------------------')\n"", 'print(list(twittercols))\n', ""print('alltwitterandcensusdata------------------------------')\n"", 'print(list(alltwitterandcensusdata.columns))\n', 'twitterdata  = alltwitterandcensusdata.loc[:,twittercols]']"
33,markdown,['# LOAD CENSUS DATA']
34,code,"[""Censuspd_TOD = pd.read_stata('Data/UDP_NYC_Variables.dta')\n"", 'Censuspd_TOD.rename(columns={\'GEOid2\': ""GEOID""},inplace=True)\n', ""Censuspd_TOD = Censuspd_TOD.loc[:,['GEOID','TOD']]\n"", 'Censuspd_TOD.head(2)']"
35,code,"['# NO GEOID\n', ""Census_data_original_NOGEOID = pd.read_csv('./Data/originalcensusfeatures.csv')\n"", ""Census_data_original_NOGEOID.drop(columns='Unnamed: 0',inplace=True)\n"", 'cols_orig = list(Census_data_original_NOGEOID.columns)\n', 'print(cols_orig)\n', 'Census_data_original_NOGEOID.head(2)']"
36,code,"[""cols_orig_noTOD = ['medrent00', 'medhval00', 'percol00', 'per_rent_00', 'pernwh00', 'hinc00', 'carcommuters_00']\n"", ""Census_data_original=pd.read_csv('NY_final_data_for_typologies_1.19.19.csv')\n"", 'Census_data_original.rename(columns={\'geoid\': ""GEOID""},inplace=True)\n', ""Census_data_original = Census_data_original.loc[:,cols_orig_noTOD+['GEOID']]\n"", ""Census_data_original.loc[:,cols_orig_noTOD+['GEOID']].head(2)""]"
37,code,"[""Census_data_new = pd.read_csv('./Data/newcensusfeatures.csv')\n"", 'Census_data_new.rename(columns={\'geoid\': ""GEOID""},inplace=True)\n', ""Census_data_new.drop(columns=['geojoin','Unnamed: 0'],inplace=True)\n"", 'Census_data_new.head(2)']"
38,code,[]
39,code,"[""Census_commercial = pd.read_csv('./Data/NYC/commercial_pct.csv')\n"", 'Census_commercial.rename(columns={\'geoid\': ""GEOID""},inplace=True)\n', ""Census_commercial.drop(columns=['Unnamed: 0'],inplace=True)\n"", 'Census_commercial.head(2)']"
40,code,"[""# Census_ages = pd.read_csv('./Data/NYC/DEC_10_SF1_QTP1_with_ann.csv')\n"", '# Census_ages.rename(columns={\'GEO.id2\': ""GEOID""},inplace=True)\n', ""# # Census_ages.drop(columns=['Unnamed: 0'],inplace=True)\n"", '# print(Census_ages.shape)\n', ""# Census_ages = Census_ages.dropna().drop('GEO.display-label',axis=1).iloc[1:,1:-4]\n"", '# print(Census_ages.shape)\n', '# Census_ages.head()']"
41,code,"['# # # pd.to_numeric[]\n', '# # Census_ages.SUBHD0101_S01.astype(float)\n', ""# # (Census_ages.SUBHD0101_S01 =='2515(r47042)').idxmax()\n"", '# # Census_ages.SUBHD0101_S01[2900]\n', '# print(Census_ages.shape)\n', '# for k, column in enumerate(Census_ages.columns):\n', ""#     print(column,'----------------------------------------')\n"", '# #     try:\n', ""#     Census_ages[column] = pd.to_numeric(Census_ages[column],errors='coerce')\n"", '# #     except:\n', '# #         for i,_ in enumerate(Census_ages[column]):\n', '# #             print(i)\n', ""# #             Census_ages.iloc[i,k] == 'nan'\n"", '# #             print(Census_ages.iloc[i,k])\n', '# #             print(""!!NULL!!"")\n', '#     Census_ages[column].replace(False,0, inplace=True)\n', ""#     Census_ages[column].replace('FALSE',0, inplace=True)\n"", ""#     Census_ages[column].replace('#NULL!',np.nan, inplace=True)\n"", '# #     Census_ages[column] = pd.to_numeric(Census_ages[column])\n', '#     for i in Census_ages[column].index:\n', ""#         if Census_ages.loc[i,column] == '#NULL!':\n"", ""#             print('#NULL',i)\n"", ""#             Census_ages['new_'+column] = 0\n"", '#         if np.isnan(Census_ages.loc[i,column]):\n', ""#             print('nan',i)\n"", ""#             Census_ages['new_'+column] = 0\n"", '#     for i in Census_ages[column].index:\n', '#         if np.isnan(Census_ages.loc[i,column]):\n', ""#             print('nan')\n"", ""#             Census_ages.loc[i,'new_'+column] = 1\n"", '\n', '#     print(column)\n', '#     Census_ages[column] = pd.to_numeric(Census_ages[column])\n', '#     Census_ages[column].replace(nan,mean(Census_ages[column]), inplace=True)\n', '\n', '# # X_foursquare.replace(False,0,inplace=True)\n', ""# # X_foursquare.replace('FALSE',0,inplace=True)\n"", ""# # X_foursquare.replace('#NULL!',0,inplace=True)\n"", '# # X_foursquare.replace(nan,0,inplace=True)\n', '\n', '# Census_ages.dropna(inplace=True)\n', '# Census_ages.shape']"
42,code,"[""Census_data_original['GEOID'] = pd.to_numeric(Census_data_original['GEOID'])\n"", ""Censuspd_TOD['GEOID'] = pd.to_numeric(Censuspd_TOD['GEOID'])\n"", ""Census_data_new['GEOID']  = pd.to_numeric(Census_data_new['GEOID'])\n"", ""# Census_ages['GEOID']  = pd.to_numeric(Census_ages['GEOID'])\n"", ""Census_commercial['GEOID']  = pd.to_numeric(Census_commercial['GEOID'])""]"
43,code,"['# Merge Censuspd_TOD, Census_data_original, Census_data_new\n', 'print(Censuspd_TOD.shape, Census_data_original.shape, Census_data_new.shape, Census_commercial.shape)\n', ""Census_new_temp = Censuspd_TOD.merge(Census_data_original,on='GEOID')\n"", 'print(Census_new_temp.shape)\n', ""Census_new_temp = Census_data_new.merge(Census_new_temp,on='GEOID')\n"", '# print(Census_new_temp.shape)\n', ""# Census_new_temp = Census_ages.merge(Census_new_temp,on='GEOID')\n"", 'print(Census_new_temp.shape)\n', ""Census_combined = Census_commercial.merge(Census_new_temp,on='GEOID')\n"", 'print(Census_combined.shape)\n', '# (Census_combined.GEOID==Census_combined.geojoin).sum()']"
44,markdown,['# Combine Datasets']
45,code,['### Combine FQ DATA::']
46,code,"[""X_foursquare = X_foursquare_high.merge(X_foursquare_neighbours, on='GEOID')\n"", 'print(X_foursquare.shape)\n', ""X_foursquare = X_foursquare.merge(X_foursquare_low, on='GEOID')\n"", 'print(X_foursquare.shape)\n', ""X_foursquare = X_foursquare.merge(X_foursquare_class, on='GEOID')\n"", 'print(X_foursquare.shape)\n', ""X_foursquare = X_foursquare.merge(X_foursquare_numeric, on='GEOID')\n"", 'print(X_foursquare.shape)\n', ""X_foursquare = X_foursquare.merge(X_foursquare_cats1, on='GEOID')\n"", 'print(X_foursquare.shape)\n', ""X_foursquare = X_foursquare.merge(X_foursquare_data_sjoined_all, on='GEOID')\n"", 'print(X_foursquare.shape)\n', 'print(X_foursquare.shape)\n', 'print(X_foursquare_low.shape[1],X_foursquare_numeric.shape[1],X_foursquare_cats1.shape[1], X_foursquare_class.shape[1],X_foursquare_high.shape[1])\n', 'X_foursquare_low.shape,X_foursquare_numeric.shape,X_foursquare_cats1.shape, X_foursquare_class.shape,X_foursquare_high.shape,X_foursquare_neighbours.shape']"
47,markdown,"['# Combine TW+FQ+Census\n', '- twitterdata \n', '- X_foursquare\n', '- Census_combined']"
48,code,"['print(X_foursquare.shape)\n', ""TW_FQpd = twitterdata.merge(X_foursquare,on='GEOID')\n"", 'print(TW_FQpd.shape)']"
49,code,"['print(TW_FQpd.shape,TW_FQpd.columns)\n', ""TW_FQpd_temp = TW_FQpd.merge(mergedgpd, on ='GEOID')\n"", 'print(TW_FQpd_temp.shape,TW_FQpd_temp.columns[:10])\n', 'TW_FQpd_temp = gpd.GeoDataFrame(TW_FQpd_temp)\n', 'figure, ax = plt.subplots()\n', ""TW_FQpd_temp.plot(column='GEOID',legend = True, ax=ax)""]"
50,code,"['print(TW_FQpd.shape)\n', ""Census_FQpd = Census_combined.merge(TW_FQpd,on='GEOID')\n"", 'print(Census_FQpd.shape)\n', 'Census_FQpd.head(1)']"
51,markdown,['## ADD GEOMETRY & TYPOLOGIES']
52,code,"['print(Census_FQpd.shape,Census_FQpd.columns)\n', ""Census_FQpd = Census_FQpd.merge(mergedgpd, on ='GEOID')\n"", 'print(Census_FQpd.shape,Census_FQpd.columns[:10])']"
53,code,['Census_FQpd.columns[-10:]']
54,code,"[""typologies = ['Type_1.19','pct_ch_percol00_16_binary','pct_ch_hinc00_16_binary',\\\n"", ""           'pct_ch_medhval00_16_binary','pct_ch_medrent00_16_binary','pct_ch_percol00_16_binary','Ongoing_adv_gent',\n"", ""                                        'gent00_16',\n"", ""                                        'gent90_00',\n"", ""                                      'Supergent16']""]"
55,code,['Census_FQpd = gpd.GeoDataFrame(Census_FQpd)']
56,code,"['print(Census_FQpd.shape)\n', 'Census_FQpd.dropna(inplace=True)\n', 'print(Census_FQpd.shape)']"
57,code,"['# visualize what is left\n', 'figure, ax = plt.subplots()\n', ""Census_FQpd.plot(column='GEOID',legend = True, ax=ax)""]"
58,code,"[""bin_typs = ['pct_ch_hinc00_16_binary',\n"", ""            'pct_ch_medhval00_16_binary','pct_ch_medrent00_16_binary','pct_ch_percol00_16_binary','Ongoing_adv_gent',\n"", ""            'gent00_16','gent90_00','Supergent16']\n"", 'for i, column in enumerate(bin_typs):\n', '    plt.figure(1)\n', '    plt.subplot(4,4,i+1)\n', ""    Census_FQpd[column].value_counts().plot(kind='bar', figsize = (15,15), title=column)\n"", '    Census_FQpd[column].value_counts()/Census_FQpd[column].value_counts().sum()']"
59,markdown,['# Part II. Modeling']
60,code,['Census_FQpd_beforedrops = Census_FQpd.copy()\n']
61,code,['binary6typ\n']
62,code,"['### Preprocessing Data\n', '\n', 'y = Census_FQpd[binary6typ]\n', ""X_Census_FQpd = Census_FQpd.drop(typologies+['GEOID','geometry'],axis =1).copy()\n"", 'print(X_Census_FQpd.shape)\n', 'X_Census_FQpd.dropna(inplace=True)\n', 'print(X_Census_FQpd.shape)']"
63,code,['X_Census_FQpd.columns']
64,code,"['# Normalize\n', '\n', 'min_max_scaler = MinMaxScaler()\n', '\n', ""models = ['Raw','Scaled','Minmaxed']\n"", 'Xdata_scaled = preprocessing.scale(X_Census_FQpd)\n', '# ydata_scaled = preprocessing.scale(y)\n', '\n', 'Xdata_minmaxed = min_max_scaler.fit_transform(X_Census_FQpd)\n', '# ydata_minmaxed = min_max_scaler.fit_transform(y)\n', '\n', 'dictx = {}\n', ""dictx['Raw'] = X_Census_FQpd\n"", ""dictx['Scaled'] = Xdata_scaled\n"", ""dictx['Minmaxed'] = Xdata_minmaxed\n"", '                    \n', '\n', '# x = dictx[model][0]\n', '# y = dictx[model][1]']"
65,code,"['model_f1_scores = {}\n', 'model_recall_scores = {}\n', 'model_precision_scores = {}\n', 'imp_dict = {}\n', 'names_dict ={}\n', 'model = {}\n', ""modelslist = ['Logit','DT','RF','SVM'] \n"", 'for m in models:\n', '    # X_train[m], X_test[m], y_train[m], y_test[m]\n', '    dictx[m]\n', '    model[m] = {}\n', '    model_f1_scores[m] = {}\n', '    model_recall_scores[m] ={}\n', '    model_precision_scores[m] = {}\n', '    imp_dict[m] = {}\n', '    names_dict[m] = {}\n', 'resultset=Census_FQpd_beforedrops.copy()']"
66,code,"['X_train = {}\n', 'X_test = {}\n', 'y_train = {}\n', 'y_test = {}\n', 'for m in models:\n', '\n', '    X_train[m], X_test[m], y_train[m], y_test[m] = train_test_split(dictx[m], y, test_size = 0.3, random_state = 1)    \n', '    #x >> dictx[m][0]\n', '    # y =>> dictx[m][1]\n', '    print(X_train[m].shape, X_test[m].shape, y_train[m].shape, y_test[m].shape)']"
67,code,[]
68,code,"['model\n', 'def f_importances_neg(coef, names,m):\n', '    \n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    imp = imp[:20]\n', '    names = names[:20]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Negative Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)  \n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_pos(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    imp = imp[-20:]\n', '    names = names[-20:]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Positive Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '#     plt.xlabel(size=8)\n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_unimp(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    lenimp_2 = len(imp)//2\n', '    imp = imp[lenimp_2-10:lenimp_2+10]\n', '    names = names[lenimp_2-10:lenimp_2+10]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_all(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Negative Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '    plt.xticks(size = 8)\n', '    plt.show()']"
69,markdown,['# LR']
70,code,"['## Logistic Reeg\n', 'imp = {}\n', 'for m in models:\n', '#     dictx[m]\n', '#     model[m]\n', '# X_train[m], X_test[m], y_train[m], y_test[m]\n', '\n', '    logit_1 = LogisticRegression(C = 10000)\n', '#     print(X_train[m].shape,y_train[m].shape)\n', '    logit_1.fit(X_train[m], y_train[m])\n', '#     print(logit_1.score(X_test[m],y_test[m]))\n', ""    model[m]['Logit'] = logit_1.score(X_test[m],y_test[m])\n"", ""#     resultset[m+'_Logit_predicttyp']=logit_1.predict(dictx[m])\n"", '    y_pred = logit_1.predict(X_test[m])\n', '    y_true = y_test[m]\n', ""    model_f1_scores[m]['Logit'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_recall_scores[m]['Logit'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_precision_scores[m]['Logit'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    f_importances_pos(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    f_importances_neg(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    f_importances_unimp(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    imp3,names = zip(*sorted(zip(list(logit_1.coef_)[0],np.asarray(list(X_train['Raw'].columns)))))\n"", ""    names_dict[m]['Logit'] = names\n"", ""    imp_dict[m]['Logit'] =  imp3\n"", ""    f_importances_all(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", 'model_f1_scores\n', 'pd.DataFrame(imp_dict)']"
71,code,"['def featureImportancePlot_dt(rf, labels,m):\n', '    importances = rf.feature_importances_[:]\n', '    indices = np.argsort(importances)[::-1]\n', '    importances_10 = rf.feature_importances_[:][:10]\n', '    indices_10 = np.argsort(importances_10)[::-1]\n', '    #std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n', '    #         axis=0)\n', '    pl.figure(figsize=(5,5))\n', '    pl.title(""Feature importances"")\n', '    pl.bar(range(indices_10.shape[0]), rf.feature_importances_[indices_10],\n', '       color=""SteelBlue"", #yerr=std[indices]\n', '           align=""center"")\n', '    pl.xticks(range(indices_10.shape[0]), np.array(labels)[indices_10], rotation=90)\n', '    pl.xlim([-1, indices_10.shape[0]])\n', '    pl.show()\n', '    print(importances) \n', '    \n', '    return rf.feature_importances_[indices], np.array(labels)[indices]']"
72,markdown,['# DT']
73,code,"['seed = 5\n', '# print(y_train.shape, X_train.shape)\n', 'for m in models:\n', '    #     dictx[m]\n', '    #     model[m]\n', '\n', '    OS = []\n', ""#     param_grid = {'n_estimators':range(1,11),'max_depth':range(1,11),'max_leaf_nodes':range(2,11)}\n"", '#     dt=DecisionTreeClassifier()\n', ""#     gr=GridSearchCV(dt,param_grid=param_grid,scoring='roc_auc')\n"", '#     ds=gr.fit(X_train[m],y_train[m])\n', '\n', '    for c in range(5):\n', ""#         print (ds.best_params_,ds.best_params_['max_depth'],ds.best_params_['max_leaf_nodes'])\n"", '        dt=DecisionTreeClassifier()\n', '        dt = DecisionTreeClassifier(max_depth=3)\n', '        dt.fit(X_train[m], y_train[m])\n', '        pred=dt.predict_proba(X_test[m])[:,1]\n', '        OS.append(dt.score(X_test[m],y_test[m]))\n', ""    model[m]['DT'] = mean(OS)\n"", ""#     resultset[m+'_DT_predicttyp']=dt.predict(dictx[m])\n"", '    y_pred = dt.predict(X_test[m])\n', '    y_true = y_test[m]\n', ""    model_f1_scores[m]['DT'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_recall_scores[m]['DT'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_precision_scores[m]['DT'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    imp_dict[m]['DT'], names_dict[m]['DT'] =  featureImportancePlot_dt(dt, X_Census_FQpd.columns,m)\n"", 'model_f1_scores']"
74,code,"[""model_f1_scores['Raw']['DT']""]"
75,code,"[""resultset['actualtyp']=y""]"
76,markdown,['# Random Forest']
77,code,"['def featureImportancePlot(rf, labels,m):\n', ""    '''plots feature importance for random forest\n"", '    rf: the random forest model fit to the data\n', '    labels: the names of the features\n', ""    '''\n"", '    \n', '    importances = rf.feature_importances_[:10]\n', '    indices = np.argsort(importances)[::-1]\n', '    std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n', '             axis=0)\n', '\n', '    pl.figure()\n', '    pl.title(""Feature importances for Model""+ m)\n', '    pl.bar(range(indices.shape[0]), rf.feature_importances_[indices],\n', '       color=""SteelBlue"", yerr=std[indices], align=""center"")\n', '    pl.xticks(range(indices.shape[0]), np.array(labels)[indices], rotation=90)\n', '    pl.xlim([-1, indices.shape[0]])\n', '    pl.show()\n', '\n', '\n', '# def featureImportancePlot(rf, labels,m):\n', ""#     '''plots feature importance for random forest\n"", '#     rf: the random forest model fit to the data\n', '#     labels: the names of the features\n', ""#     '''\n"", '#     importances = rf.feature_importances_\n', '#     indices = np.argsort(importances)[::-1]\n', '#     importances_10 = rf.feature_importances_[:10]\n', '#     indices_10 = np.argsort(importances_10)[::-1]\n', '#     std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n', '#              axis=0)\n', '\n', '#     pl.figure(figsize=(5,5))\n', '#     pl.title(""Feature importances of Random Forest for Model ""+m)\n', '#     pl.bar(range(indices_10.shape[0]), rf.feature_importances_[indices_10],\n', '#        color=""SteelBlue"", yerr=std[indices_10], align=""center"")\n', '#     pl.xticks(range(indices_10.shape[0]), np.array(labels)[indices_10], rotation=90)\n', '#     pl.xlim([-1, indices_10.shape[0]])\n', '#     pl.show()\n', '#     return rf.feature_importances_[indices], np.array(labels)[indices]']"
78,code,"['for m in models:\n', '    #     dictx[m]\n', '    print(m)\n', ""    param_grid = {'max_depth':range(1,11),'n_estimators':range(1,11),'max_leaf_nodes':range(2,11)}\n"", '    rf=RandomForestClassifier()\n', ""    gr=GridSearchCV(rf,param_grid=param_grid,scoring='roc_auc')\n"", '    rs=gr.fit(X_train[m],y_train[m])\n', '    OS = []\n', '    for c in range(5):\n', ""        print(rs.best_params_,rs.best_params_['max_depth'],rs.best_params_['max_leaf_nodes'])\n"", ""        rf = RandomForestClassifier(max_depth=rs.best_params_['max_depth'],max_leaf_nodes=rs.best_params_['max_leaf_nodes'])\n"", '        rf.fit(X_train[m], y_train[m])\n', '        pred=rf.predict_proba(X_test[m])[:,1]\n', '        OS.append(rf.score(X_test[m],y_test[m]))\n', ""    model[m]['RF'] = mean(OS)\n"", ""    resultset[m+'_RF_predicttyp']=rf.predict(dictx[m])\n"", '    y_pred = rf.predict(X_test[m])\n', '    y_true = y_test[m]\n', ""    model_f1_scores[m]['RF'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_recall_scores[m]['RF'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_precision_scores[m]['RF'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    imp_dict[m]['RF'], names_dict[m]['RF'] =  featureImportancePlot_dt(dt, X_Census_FQpd.columns,m)\n"", 'model_f1_scores']"
79,markdown,['#### Note that the features have changed considerably with the updated binary typology-- users and checkins were the most important features initially.']
80,markdown,['# SVM']
81,code,"['model\n', 'def f_importances_neg(coef, names,m):\n', '    \n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    imp = imp[:20]\n', '    names = names[:20]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Negative Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)  \n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_pos(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    imp = imp[-20:]\n', '    names = names[-20:]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Positive Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '#     plt.xlabel(size=8)\n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_unimp(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    lenimp_2 = len(imp)//2\n', '    imp = imp[lenimp_2-10:lenimp_2+10]\n', '    names = names[lenimp_2-10:lenimp_2+10]\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '    plt.xticks(size = 8)\n', '    plt.show()\n', 'def f_importances_all(coef, names,m):\n', '    imp = coef\n', '    print((imp.shape))\n', '    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n', '    plt.figure(figsize=(5,5))\n', ""    plt.title('Negative Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n"", ""    plt.barh(range(len(names)), imp, align='center')\n"", '    plt.yticks(range(len(names)), names)\n', '#     plt.xlabel(size=8)\n', '    plt.xticks(size = 8)\n', '    plt.show()\n', '\n']"
82,code,"['\n', '# When C is very small, we are willing to tolerate more mistakes. If C is very big, this\n', '# means we hardly tolerate any mistakes. So, we cannot choose a very large C if our data is not\n', ""# really separable. Let's however choose from a broad range of reasonable options.\n"", ""# param_grid = {'kernel':['linear'],'C':[np.exp(i) for i in np.linspace(-10,10,10)]}\n"", 'for m in models:\n', '    print(m)\n', '    OS = []\n', '    #     dictx[m]\n', '        #     model[m]\n', ""    rr = svm.SVC(gamma='auto')\n"", '    rr.fit(X_train[m], y_train[m])\n', '    correct=1.0*(rr.predict(X_test[m])==np.asarray(y_test[m])).sum()/len(y_test[m])\n', '    print(correct)\n', '    print(rr.score(X_test[m],y_test[m]))\n', '    OS.append(correct)\n', ""#     resultset[m+'_SVM_predicttyp']=rr.predict(dictx[m])\n"", ""    model[m]['SVM'] = mean(OS)\n"", '    y_pred = rf.predict(X_test[m])\n', '    y_true = y_test[m]\n', ""    model_f1_scores[m]['SVM'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_recall_scores[m]['SVM'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    model_precision_scores[m]['SVM'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n"", ""    f_importances_pos(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    f_importances_neg(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    f_importances_unimp(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n"", ""    imp3,names = zip(*sorted(zip(list(logit_1.coef_)[0],np.asarray(list(X_train['Raw'].columns)))))\n"", ""    names_dict[m]['SVM'] = names\n"", ""    imp_dict[m]['SVM'] = imp3 \n"", 'model_f1_scores']"
83,markdown,['# Summary of Results']
84,code,['X_Census_FQpd.columns']
85,code,['X_Census_FQpd.shape']
86,code,[]
87,code,"['df1 = pd.DataFrame(model_f1_scores)\n', 'df2 = pd.DataFrame(model_recall_scores)\n', 'df3 =pd.DataFrame(model_precision_scores)\n', 'result = pd.concat([df1, df2,df3], axis=1, sort=False)\n', 'result']"
88,code,['model_name']
89,code,"[""result.to_csv('./Results/Scores'+model_name+'.csv')""]"
90,code,"['names_pd = pd.DataFrame()\n', 'counter = {}\n', 'for m in models:\n', '    print(m)\n', '    for standarized in modelslist:\n', '        for i, weights in enumerate(names_dict[m][standarized]):\n', ""            standarized_m = str(standarized)+'_'+str(m)\n"", '            names_pd.loc[standarized_m,weights] = imp_dict[m][standarized][i]\n']"
91,code,"['for i in names_pd.index:\n', '    print(names_pd.loc[i].idxmax())']"
92,code,['model_name']
93,code,"[""names_pd.to_csv('./Results/'+model_name+'.csv')\n"", 'names_pd']"
94,code,[]
95,code,[]
96,code,[]
97,markdown,['- You can refer to https://github.com/mv1742/updny_2']
