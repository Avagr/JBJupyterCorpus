cell_id,cell_type,source
0,markdown,"['# CNTK 207: Sampled Softmax\n', '\n', 'For classification and prediction problems a typical criterion function is cross-entropy with softmax. If the number of output classes is high the computation of this criterion and the corresponding gradients could be quite costly. Sampled Softmax is a heuristic to speed up training in these cases. (see: [Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model](http://www.iro.umontreal.ca/~lisa/pointeurs/importance_samplingIEEEtnn.pdf), [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410v1.pdf), [What is Candidate Sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf))\n']"
1,markdown,"['**Select the notebook runtime environment devices / settings**\n', '\n', 'Before we dive into the details we run some setup that is required for automated testing of this notebook. \n']"
2,code,"['from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n', 'from __future__ import division\n', '\n', 'import os\n', 'import cntk as C\n', 'import cntk.tests.test_utils\n', 'cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n', 'C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components']"
3,markdown,"['## Basic concept\n', '\n', 'The softmax function is used in neural networks if we want to interpret the network output as a probability distribution over a set of classes $C$ with $|C|=N_C$.\n', '\n', 'Softmax maps an $N_C$-dimensional vector $z$, which has unrestricted values, to an $N_C$ dimensional vector $p$ with non-negative values that sum up to 1 so that they can be interpreted as probabilities. More precisely:\n', '\n', '$$\n', '\\begin{align}\n', 'p_i &= softmax(z, i)\\\\\n', '    &= \\frac{exp(z_i)}{\\sum_{k\\in C} exp(z_k)}\\\\\n', '\\end{align}\n', '$$\n', '\n', 'In what follows we assume that the input $z$ to the softmax is computed from some hidden vector $h$ of dimension $N_h$  in a specific way, namely:\n', '\n', '$$ z = W h + b $$\n', '\n', 'where $W$ is a learnable weight matrix of dimension $(N_c, N_h)$ and $b$ is a learnable bias vector.\n', 'We restrict ourselves to this specific choice of $z$ because it helps in implementing an efficient sampled softmax.\n', '\n', 'In a typical use-case like for example a recurrent language model, the hidden vector $h$ would be the output of the recurrent layers and $C$ would be the set of words to predict.   \n', '\n', 'As a training criterion, we use cross-entropy which is a function of the expected (true) class $t\\in C$ and the probability predicted for it:\n', '\n', '$$cross\\_entropy := -log(p_t)$$\n', '\n', '## Sampled Softmax\n', '\n', 'For the normal softmax the CNTK Python-api provides the function [cross_entropy_with_softmax](https://cntk.ai/pythondocs/cntk.ops.html?highlight=softmax#cntk.ops.cross_entropy_with_softmax). This takes as input the $N_C$-dimensional vector $z$. As mentioned for our sampled softmax implementation we assume that this z is computed by $ z = W h + b $. In sampled softmax this has to be part of the whole implementation of the criterion.\n', '\n', 'Below we show the code for `cross_entropy_with_sampled_softmax_and_embedding`. Letâ€™s look at the signature first.\n', '\n', 'One fundamental difference to the corresponding function in the Python-api (`cross_entropy_with_softmax`) is that in the Python api function the input corresponds to $z$ and must have the same dimension as the target vector, while in cross_entropy_with_full_softmax the input corresponds to our hidden vector $h$ can have any dimension (hidden_dim).\n', 'Actually, hidden_dim will be typically much lower than the dimension of the target vector.\n', '\n', 'We also have some additional parameters `num_samples, sampling_weights, allow_duplicates` that control the random sampling. \n', 'Another difference to the api function is that we return a triple (z, cross_entropy_on_samples, error_on_samples).\n', '\n', 'We will come back to the details of the implementation below.']"
4,code,"['# Creates a subgraph computing cross-entropy with sampled softmax.\n', 'def cross_entropy_with_sampled_softmax_and_embedding(\n', '    hidden_vector,            # Node providing hidden input\n', '    target_vector,            # Node providing the expected labels (as sparse vectors)\n', '    num_classes,              # Number of classes\n', '    hidden_dim,               # Dimension of the hidden vector\n', '    num_samples,              # Number of samples to use for sampled softmax\n', '    sampling_weights,         # Node providing weights to be used for the weighted sampling\n', '    allow_duplicates = True,  # Boolean flag to control whether to use sampling with replacemement \n', '                              # (allow_duplicates == True) or without replacement.\n', '    ):\n', '    # define the parameters learnable parameters\n', '    b = C.Parameter(shape = (num_classes, 1), init = 0)\n', '    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.glorot_uniform())\n', '\n', '    # Define the node that generates a set of random samples per minibatch\n', '    # Sparse matrix (num_samples * num_classes)\n', '    sample_selector = C.random_sample(sampling_weights, num_samples, allow_duplicates)\n', '\n', '    # For each of the samples we also need the probablity that it in the sampled set.\n', '    inclusion_probs = C.random_sample_inclusion_frequency(sampling_weights, num_samples, allow_duplicates) # dense row [1 * vocab_size]\n', '    log_prior = C.log(inclusion_probs) # dense row [1 * num_classes]\n', '\n', ""    # Create a submatrix wS of 'weights\n"", '    W_sampled = C.times(sample_selector, W) # [num_samples * hidden_dim]\n', '    z_sampled = C.times_transpose(W_sampled, hidden_vector) + C.times(sample_selector, b) - C.times_transpose (sample_selector, log_prior)# [num_samples]\n', '\n', '    # Getting the weight vector for the true label. Dimension hidden_dim\n', '    W_target = C.times(target_vector, W) # [1 * hidden_dim]\n', '    z_target = C.times_transpose(W_target, hidden_vector) + C.times(target_vector, b) - C.times_transpose(target_vector, log_prior) # [1]\n', '\n', '\n', '    z_reduced = C.reduce_log_sum_exp(z_sampled)\n', '    \n', '    # Compute the cross entropy that is used for training.\n', ""    # We don't check whether any of the classes in the random samples conincides with the true label, so it might\n"", '    # happen that the true class is counted\n', '    # twice in the normalising demnominator of sampled softmax.\n', '    cross_entropy_on_samples = C.log_add_exp(z_target, z_reduced) - z_target\n', '\n', '    # For applying the model we also output a node providing the input for the full softmax\n', '    z = C.times_transpose(W, hidden_vector) + b\n', '    z = C.reshape(z, shape = (num_classes))\n', '\n', '    zSMax = C.reduce_max(z_sampled)\n', '    error_on_samples = C.less(z_target, zSMax)\n', '    return (z, cross_entropy_on_samples, error_on_samples)']"
5,markdown,['To give a better idea of what the inputs and outputs are and how this all differs from the normal softmax we give below a corresponding function using normal softmax:']
6,code,"['# Creates subgraph computing cross-entropy with (full) softmax.\n', 'def cross_entropy_with_softmax_and_embedding(\n', '    hidden_vector,  # Node providing hidden input\n', '    target_vector,  # Node providing the expected labels (as sparse vectors)\n', '    num_classes,    # Number of classes\n', '    hidden_dim      # Dimension of the hidden vector\n', '    ):\n', '    # Setup bias and weights\n', '    b = C.Parameter(shape = (num_classes, 1), init = 0)\n', '    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.glorot_uniform())\n', '\n', '    \n', '    z = C.reshape( C.times_transpose(W, hidden_vector) + b, (1, num_classes))\n', '    \n', '    # Use cross_entropy_with_softmax\n', '    cross_entropy = C.cross_entropy_with_softmax(z, target_vector)\n', '\n', '    zMax = C.reduce_max(z)\n', '    zT = C.times_transpose(z, target_vector)\n', '    error_on_samples = C.less(zT, zMax)\n', '\n', '    return (z, cross_entropy, error_on_samples)']"
7,markdown,"['As you can see the main differences to the api function `cross_entropy_with_softmax` are:\n', '* We include the mapping $ z = W h + b $ into the function.\n', '* We return a triple (z, cross_entropy, error_on_samples) instead of just returning the cross entropy.\n', '\n', '\n', '## A toy example\n', '\n', 'To explain how to integrate sampled softmax let us look at a toy example. In this toy example we first transform one-hot input vectors via some random projection into a lower dimensional vector $h$. The modeling task is to reverse this mapping using (sampled) softmax. Well, as already said this is a toy example.\n']"
8,code,"['import numpy as np\n', 'from math import log, exp, sqrt\n', 'from cntk.logging import ProgressPrinter\n', 'import timeit\n', '\n', '# A class with all parameters\n', 'class Param:\n', '    # Learning parameters\n', '    learning_rate = 0.03\n', '    minibatch_size = 100\n', '    num_minbatches = 100\n', '    test_set_size = 1000\n', '    momentum = 0.8187307530779818\n', '    reporting_interval = 10\n', '    allow_duplicates = False\n', '    \n', '    # Parameters for sampled softmax\n', '    use_sampled_softmax = True\n', '    use_sparse = True\n', '    softmax_sample_size = 10\n', '\n', '    # Details of data and model\n', '    num_classes = 50\n', '    hidden_dim = 10\n', '    \n', 'data_sampling_distribution = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '    \n', 'softmax_sampling_weights = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '\n', ""# Creates random one-hot vectors of dimension 'num_classes'.\n"", '# Returns a tuple with a list of one-hot vectors, and list with the indices they encode.\n', 'def get_random_one_hot_data(num_vectors):\n', '    indices = np.random.choice(\n', '        range(Param.num_classes),\n', '        size=num_vectors, \n', '        p = data_sampling_distribution()).reshape((num_vectors, 1))\n', '    list_of_vectors = C.Value.one_hot(indices, Param.num_classes)\n', '    return (list_of_vectors, indices.flatten())\n', '\n', '# Create a network that:\n', '# * Transforms the input one hot-vectors with a constant random embedding\n', '# * Applies a linear decoding with parameters we want to learn\n', 'def create_model(labels):\n', '    # random projection matrix\n', '    random_data = np.random.normal(scale = sqrt(1.0/Param.hidden_dim), size=(Param.num_classes, Param.hidden_dim)).astype(np.float32)\n', '    random_matrix = C.constant(shape = (Param.num_classes, Param.hidden_dim), value = random_data)\n', '    \n', '    h = C.times(labels, random_matrix)\n', '    \n', '    # Connect the latent output to (sampled/full) softmax.\n', '    if Param.use_sampled_softmax:\n', '        sampling_weights = np.asarray(softmax_sampling_weights(), dtype=np.float32)\n', '        sampling_weights.reshape((1, Param.num_classes))\n', '        softmax_input, ce, errs = cross_entropy_with_sampled_softmax_and_embedding(\n', '            h, \n', '            labels,\n', '            Param.num_classes, \n', '            Param.hidden_dim, \n', '            Param.softmax_sample_size, \n', '            softmax_sampling_weights(),\n', '            Param.allow_duplicates)\n', '    else:\n', '        softmax_input, ce, errs = cross_entropy_with_softmax_and_embedding(\n', '            h, \n', '            labels, \n', '            Param.num_classes, \n', '            Param.hidden_dim)\n', '\n', '    return softmax_input, ce, errs\n', '\n', 'def train(do_print_progress):\n', '    labels = C.input_variable(shape = Param.num_classes, is_sparse = Param.use_sparse)\n', '    z, cross_entropy, errs = create_model(labels)\n', '\n', '    # Setup the trainer\n', '    learning_rate_schedule = C.learning_parameter_schedule_per_sample(Param.learning_rate)\n', '    momentum_schedule = C.momentum_schedule(Param.momentum, minibatch_size = Param.minibatch_size)\n', '    learner = C.momentum_sgd(z.parameters, learning_rate_schedule, momentum_schedule, True)\n', '    progress_writers = None\n', '    if do_print_progress:\n', ""        progress_writers = [ProgressPrinter(freq=Param.reporting_interval, tag='Training')]\n"", '    trainer = C.Trainer(z, (cross_entropy, errs), learner, progress_writers)\n', '\n', '    minbatch = 0\n', '    average_cross_entropy = compute_average_cross_entropy(z)\n', '    minbatch_data = [0] # store minibatch values\n', '    cross_entropy_data = [average_cross_entropy] # store cross_entropy values\n', '\n', '    # Run training\n', '    t_total= 0\n', '\n', '    # Run training\n', '    for minbatch in range(1,Param.num_minbatches):\n', '        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n', '        label_data, indices = get_random_one_hot_data(Param.minibatch_size)\n', '        arguments = ({labels : label_data})\n', '\n', '        # If do_print_progress is True, this will automatically print the progress using ProgressPrinter\n', '        # The printed loss numbers are computed using the sampled softmax criterion\n', '        t_start = timeit.default_timer()\n', '        trainer.train_minibatch(arguments)\n', '        t_end = timeit.default_timer()\n', '\n', '        t_delta = t_end - t_start\n', '        samples_per_second = Param.minibatch_size / t_delta\n', '        \n', '        # We ignore the time measurements of the first two minibatches\n', '        if minbatch > 2:\n', '            t_total += t_delta\n', '\n', '        # For comparison also print result using the full criterion\n', '        if minbatch % Param.reporting_interval == int(Param.reporting_interval/2):\n', '            # memorize the progress data for plotting\n', '            average_cross_entropy = compute_average_cross_entropy(z)\n', '            minbatch_data.append(minbatch)\n', '            cross_entropy_data.append(average_cross_entropy)\n', '            \n', '            if do_print_progress:\n', '                print(""\\nMinbatch=%d Cross-entropy from full softmax = %.3f perplexity = %.3f samples/s = %.1f""\n', '                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n', '                \n', '    # Number of samples we measured. First two minbatches were ignored\n', '    samples_measured = Param.minibatch_size * (Param.num_minbatches - 2)\n', '    overall_samples_per_second = samples_measured / t_total\n', '    return (minbatch_data, cross_entropy_data, overall_samples_per_second) \n', '\n', 'def compute_average_cross_entropy(softmax_input):\n', '    vectors, indices = get_random_one_hot_data(Param.test_set_size)\n', '    total_cross_entropy = 0.0\n', '    arguments = (vectors)\n', '    z = softmax_input.eval(arguments).reshape(Param.test_set_size, Param.num_classes)\n', '\n', '    for i in range(len(indices)):\n', '        log_p = log_softmax(z[i], indices[i])\n', '        total_cross_entropy -= log_p\n', '\n', '    return total_cross_entropy / len(indices)\n', '\n', '# Computes log(softmax(z,index)) for a one-dimensional numpy array z in an numerically stable way.\n', 'def log_softmax(z,    # numpy array\n', '                index # index into the array\n', '            ):\n', '    max_z = np.max(z)\n', '    return z[index] - max_z - log(np.sum(np.exp(z - max_z)))\n', '\n', '\n', '\n', 'np.random.seed(1)\n', '\n', 'print(""start..."")\n', 'train(do_print_progress = True)\n', 'print(""done."")']"
9,markdown,"['In the above code we use two different methods to report training progress:\n', '1. Using a function that computes the average cross entropy on full softmax.\n', '2. Using the built-in ProgressPrinter\n', '\n', 'ProgressPrinter reports how the value of the training criterion changes over time.\n', 'In our case the training criterion is cross-entropy from **sampled** softmax.\n', 'The same is true for the error rate computed by progress printer, this is computed only for true-class vs sampled-classes and will therefore underestimate the true error rate.\n', '\n', 'Therefore while ProgressPrinter already gives us some idea how training goes on, if we want to compare the behavior for different sampling strategies (sample size, sampling weights, ...) we should not rely on numbers that are computed only using the sampled subset of classes. \n', '\n', '\n', '## Importance sampling\n', '\n', ""Often the we don't have uniform distribution for the classes on the output side. The typical example is when we have words as output classes. A typical example are words where e.g. 'the' will be much more frequent than most others.\n"", '\n', 'In such cases one often uses a non uniform distribution for drawing the samples in sampled softmax but instead increases the sampling weight for the frequent classes. This is also called importane sampling.\n', 'In our example the sampling distribution is controlled by the weight array `softmax_sampling_weights`.\n', '\n', ""As an example let's look at the case where the classes are distrubted according to zipf-distrubtion like:\n"", '$$\n', 'p[i] \\propto \\frac{1}{i+5},\n', '$$\n', 'actually we use this distribution already in our example.\n', '\n', 'How does training behavior change if we switch uniform sampling to sampling with the zipfian distribution in sampled softmax?\n', '\n', '\n', '\n', '\n', '\n']"
10,code,"['# We want to lot the data \n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', '\n', '# Define weights of zipfian distributuion\n', 'def zipf(index):\n', '    return 1.0 / (index + 5)\n', '\n', '# Use zipifian distribution for the classes\n', 'def zipf_sampling_weights():\n', '    return np.asarray([ zipf(i) for i in range(Param.num_classes)], dtype=np.float32)\n', '\n', 'data_sampling_distribution = lambda: zipf_sampling_weights() / np.sum(zipf_sampling_weights())\n', '\n', 'print(""start..."")\n', '\n', '\n', '# Train using uniform sampling (like before)\n', 'np.random.seed(1)\n', 'softmax_sampling_weights = lambda: np.repeat(1.0/Param.num_classes, Param.num_classes)\n', 'minibatch_data, cross_entropy_data, _ = train(do_print_progress = False)\n', '\n', '# Train using importance sampling\n', 'np.random.seed(1)\n', 'softmax_sampling_weights = zipf_sampling_weights\n', 'minibatch_data2, cross_entropy_data2, _ = train(do_print_progress = False)\n', '\n', ""plt.plot(minibatch_data, cross_entropy_data, 'r--',minibatch_data, cross_entropy_data2, 'b--')\n"", ""plt.xlabel('number of mini-batches')\n"", ""plt.ylabel('cross entropy')\n"", 'plt.show()']"
11,markdown,"['In the example above we compare uniform sampling (red) vs sampling with the same distribution the classes have (blue).\n', 'You will need to experiment to find the best settings for all the softmax parameters.\n']"
12,markdown,"['## What speedups to expect?\n', '\n', 'The speed difference between full softmax and sampled softmax in terms of training instances depends strongly on the concrete settings, namely\n', '* Number of classes. Typically the speed-up will increase the more output classes you have.\n', '* Number of samples used in sampled softmax\n', '* Dimension of hiddlen layer input\n', '* Minibatch size\n', '* Hardware\n', '\n', 'Also you need to test how much you can reduce sample size without degradation of the result.']"
13,code,"['print(""start..."")\n', '\n', '# Reset parameters\n', 'class Param:\n', '    # Learning parameters\n', '    learning_rate = 0.03\n', '    minibatch_size = 8\n', '    num_minbatches = 100\n', '    test_set_size = 1 # we are only interrested in speed\n', '    momentum = 0.8187307530779818\n', '    reporting_interval = 1000000 # Switch off reporting to speed up\n', '    allow_duplicates = False\n', '    \n', '    # Parameters for sampled softmax\n', '    use_sampled_softmax = True\n', '    use_sparse = True\n', '    softmax_sample_size = 10\n', '\n', '    # Details of data and model\n', '    num_classes = 50000\n', '    hidden_dim = 10\n', '    \n', 'data_sampling_distribution = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', 'softmax_sampling_weights = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '\n', '    \n', 'sample_sizes = [5, 10, 100, 1000]\n', 'speed_with_sampled_softmax = []\n', '\n', '# Get the speed with sampled softmax for different sizes\n', 'for sample_size in sample_sizes: \n', '    print(""Measuring speed of sampled softmax for sample size %d ..."" % (sample_size))\n', '    Param.use_sampled_softmax = True\n', '    Param.softmax_sample_size = sample_size\n', '    _, _,  samples_per_second = train(do_print_progress = False)\n', '    speed_with_sampled_softmax.append(samples_per_second)\n', '\n', '# Get the speed with full softmax\n', 'Param.use_sampled_softmax = False\n', 'print(""Measuring speed of full softmax ..."")\n', '_, _,  samples_per_second = train(do_print_progress = False)\n', 'speed_without_sampled_softmax = np.repeat(samples_per_second, len(sample_sizes))\n', '\n', '# Plot the speed of sampled softmax (blue) as a function of sample sizes\n', '# and compare it to the speed with full softmax (red).    \n', ""plt.plot(sample_sizes, speed_without_sampled_softmax, 'r--',sample_sizes, speed_with_sampled_softmax, 'b--')\n"", ""plt.xlabel('softmax sample size')\n"", ""plt.ylabel('speed: instances / second')\n"", 'plt.title(""Speed \'sampled softmax\' (blue) vs. \'full softmax\' (red)"")\n', 'plt.ylim(ymin=0)\n', 'plt.show()']"
14,code,[]
