cell_id,source,execution_count
2,"['from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n', 'from __future__ import division\n', '\n', 'import os\n', 'import cntk as C\n', 'import cntk.tests.test_utils\n', 'cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n', 'C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components']",1.0
4,"['# Creates a subgraph computing cross-entropy with sampled softmax.\n', 'def cross_entropy_with_sampled_softmax_and_embedding(\n', '    hidden_vector,            # Node providing hidden input\n', '    target_vector,            # Node providing the expected labels (as sparse vectors)\n', '    num_classes,              # Number of classes\n', '    hidden_dim,               # Dimension of the hidden vector\n', '    num_samples,              # Number of samples to use for sampled softmax\n', '    sampling_weights,         # Node providing weights to be used for the weighted sampling\n', '    allow_duplicates = True,  # Boolean flag to control whether to use sampling with replacemement \n', '                              # (allow_duplicates == True) or without replacement.\n', '    ):\n', '    # define the parameters learnable parameters\n', '    b = C.Parameter(shape = (num_classes, 1), init = 0)\n', '    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.glorot_uniform())\n', '\n', '    # Define the node that generates a set of random samples per minibatch\n', '    # Sparse matrix (num_samples * num_classes)\n', '    sample_selector = C.random_sample(sampling_weights, num_samples, allow_duplicates)\n', '\n', '    # For each of the samples we also need the probablity that it in the sampled set.\n', '    inclusion_probs = C.random_sample_inclusion_frequency(sampling_weights, num_samples, allow_duplicates) # dense row [1 * vocab_size]\n', '    log_prior = C.log(inclusion_probs) # dense row [1 * num_classes]\n', '\n', ""    # Create a submatrix wS of 'weights\n"", '    W_sampled = C.times(sample_selector, W) # [num_samples * hidden_dim]\n', '    z_sampled = C.times_transpose(W_sampled, hidden_vector) + C.times(sample_selector, b) - C.times_transpose (sample_selector, log_prior)# [num_samples]\n', '\n', '    # Getting the weight vector for the true label. Dimension hidden_dim\n', '    W_target = C.times(target_vector, W) # [1 * hidden_dim]\n', '    z_target = C.times_transpose(W_target, hidden_vector) + C.times(target_vector, b) - C.times_transpose(target_vector, log_prior) # [1]\n', '\n', '\n', '    z_reduced = C.reduce_log_sum_exp(z_sampled)\n', '    \n', '    # Compute the cross entropy that is used for training.\n', ""    # We don't check whether any of the classes in the random samples conincides with the true label, so it might\n"", '    # happen that the true class is counted\n', '    # twice in the normalising demnominator of sampled softmax.\n', '    cross_entropy_on_samples = C.log_add_exp(z_target, z_reduced) - z_target\n', '\n', '    # For applying the model we also output a node providing the input for the full softmax\n', '    z = C.times_transpose(W, hidden_vector) + b\n', '    z = C.reshape(z, shape = (num_classes))\n', '\n', '    zSMax = C.reduce_max(z_sampled)\n', '    error_on_samples = C.less(z_target, zSMax)\n', '    return (z, cross_entropy_on_samples, error_on_samples)']",2.0
6,"['# Creates subgraph computing cross-entropy with (full) softmax.\n', 'def cross_entropy_with_softmax_and_embedding(\n', '    hidden_vector,  # Node providing hidden input\n', '    target_vector,  # Node providing the expected labels (as sparse vectors)\n', '    num_classes,    # Number of classes\n', '    hidden_dim      # Dimension of the hidden vector\n', '    ):\n', '    # Setup bias and weights\n', '    b = C.Parameter(shape = (num_classes, 1), init = 0)\n', '    W = C.Parameter(shape = (num_classes, hidden_dim), init = C.glorot_uniform())\n', '\n', '    \n', '    z = C.reshape( C.times_transpose(W, hidden_vector) + b, (1, num_classes))\n', '    \n', '    # Use cross_entropy_with_softmax\n', '    cross_entropy = C.cross_entropy_with_softmax(z, target_vector)\n', '\n', '    zMax = C.reduce_max(z)\n', '    zT = C.times_transpose(z, target_vector)\n', '    error_on_samples = C.less(zT, zMax)\n', '\n', '    return (z, cross_entropy, error_on_samples)']",3.0
8,"['import numpy as np\n', 'from math import log, exp, sqrt\n', 'from cntk.logging import ProgressPrinter\n', 'import timeit\n', '\n', '# A class with all parameters\n', 'class Param:\n', '    # Learning parameters\n', '    learning_rate = 0.03\n', '    minibatch_size = 100\n', '    num_minbatches = 100\n', '    test_set_size = 1000\n', '    momentum = 0.8187307530779818\n', '    reporting_interval = 10\n', '    allow_duplicates = False\n', '    \n', '    # Parameters for sampled softmax\n', '    use_sampled_softmax = True\n', '    use_sparse = True\n', '    softmax_sample_size = 10\n', '\n', '    # Details of data and model\n', '    num_classes = 50\n', '    hidden_dim = 10\n', '    \n', 'data_sampling_distribution = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '    \n', 'softmax_sampling_weights = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '\n', ""# Creates random one-hot vectors of dimension 'num_classes'.\n"", '# Returns a tuple with a list of one-hot vectors, and list with the indices they encode.\n', 'def get_random_one_hot_data(num_vectors):\n', '    indices = np.random.choice(\n', '        range(Param.num_classes),\n', '        size=num_vectors, \n', '        p = data_sampling_distribution()).reshape((num_vectors, 1))\n', '    list_of_vectors = C.Value.one_hot(indices, Param.num_classes)\n', '    return (list_of_vectors, indices.flatten())\n', '\n', '# Create a network that:\n', '# * Transforms the input one hot-vectors with a constant random embedding\n', '# * Applies a linear decoding with parameters we want to learn\n', 'def create_model(labels):\n', '    # random projection matrix\n', '    random_data = np.random.normal(scale = sqrt(1.0/Param.hidden_dim), size=(Param.num_classes, Param.hidden_dim)).astype(np.float32)\n', '    random_matrix = C.constant(shape = (Param.num_classes, Param.hidden_dim), value = random_data)\n', '    \n', '    h = C.times(labels, random_matrix)\n', '    \n', '    # Connect the latent output to (sampled/full) softmax.\n', '    if Param.use_sampled_softmax:\n', '        sampling_weights = np.asarray(softmax_sampling_weights(), dtype=np.float32)\n', '        sampling_weights.reshape((1, Param.num_classes))\n', '        softmax_input, ce, errs = cross_entropy_with_sampled_softmax_and_embedding(\n', '            h, \n', '            labels,\n', '            Param.num_classes, \n', '            Param.hidden_dim, \n', '            Param.softmax_sample_size, \n', '            softmax_sampling_weights(),\n', '            Param.allow_duplicates)\n', '    else:\n', '        softmax_input, ce, errs = cross_entropy_with_softmax_and_embedding(\n', '            h, \n', '            labels, \n', '            Param.num_classes, \n', '            Param.hidden_dim)\n', '\n', '    return softmax_input, ce, errs\n', '\n', 'def train(do_print_progress):\n', '    labels = C.input_variable(shape = Param.num_classes, is_sparse = Param.use_sparse)\n', '    z, cross_entropy, errs = create_model(labels)\n', '\n', '    # Setup the trainer\n', '    learning_rate_schedule = C.learning_parameter_schedule_per_sample(Param.learning_rate)\n', '    momentum_schedule = C.momentum_schedule(Param.momentum, minibatch_size = Param.minibatch_size)\n', '    learner = C.momentum_sgd(z.parameters, learning_rate_schedule, momentum_schedule, True)\n', '    progress_writers = None\n', '    if do_print_progress:\n', ""        progress_writers = [ProgressPrinter(freq=Param.reporting_interval, tag='Training')]\n"", '    trainer = C.Trainer(z, (cross_entropy, errs), learner, progress_writers)\n', '\n', '    minbatch = 0\n', '    average_cross_entropy = compute_average_cross_entropy(z)\n', '    minbatch_data = [0] # store minibatch values\n', '    cross_entropy_data = [average_cross_entropy] # store cross_entropy values\n', '\n', '    # Run training\n', '    t_total= 0\n', '\n', '    # Run training\n', '    for minbatch in range(1,Param.num_minbatches):\n', '        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n', '        label_data, indices = get_random_one_hot_data(Param.minibatch_size)\n', '        arguments = ({labels : label_data})\n', '\n', '        # If do_print_progress is True, this will automatically print the progress using ProgressPrinter\n', '        # The printed loss numbers are computed using the sampled softmax criterion\n', '        t_start = timeit.default_timer()\n', '        trainer.train_minibatch(arguments)\n', '        t_end = timeit.default_timer()\n', '\n', '        t_delta = t_end - t_start\n', '        samples_per_second = Param.minibatch_size / t_delta\n', '        \n', '        # We ignore the time measurements of the first two minibatches\n', '        if minbatch > 2:\n', '            t_total += t_delta\n', '\n', '        # For comparison also print result using the full criterion\n', '        if minbatch % Param.reporting_interval == int(Param.reporting_interval/2):\n', '            # memorize the progress data for plotting\n', '            average_cross_entropy = compute_average_cross_entropy(z)\n', '            minbatch_data.append(minbatch)\n', '            cross_entropy_data.append(average_cross_entropy)\n', '            \n', '            if do_print_progress:\n', '                print(""\\nMinbatch=%d Cross-entropy from full softmax = %.3f perplexity = %.3f samples/s = %.1f""\n', '                    % (minbatch, average_cross_entropy, exp(average_cross_entropy), samples_per_second))\n', '                \n', '    # Number of samples we measured. First two minbatches were ignored\n', '    samples_measured = Param.minibatch_size * (Param.num_minbatches - 2)\n', '    overall_samples_per_second = samples_measured / t_total\n', '    return (minbatch_data, cross_entropy_data, overall_samples_per_second) \n', '\n', 'def compute_average_cross_entropy(softmax_input):\n', '    vectors, indices = get_random_one_hot_data(Param.test_set_size)\n', '    total_cross_entropy = 0.0\n', '    arguments = (vectors)\n', '    z = softmax_input.eval(arguments).reshape(Param.test_set_size, Param.num_classes)\n', '\n', '    for i in range(len(indices)):\n', '        log_p = log_softmax(z[i], indices[i])\n', '        total_cross_entropy -= log_p\n', '\n', '    return total_cross_entropy / len(indices)\n', '\n', '# Computes log(softmax(z,index)) for a one-dimensional numpy array z in an numerically stable way.\n', 'def log_softmax(z,    # numpy array\n', '                index # index into the array\n', '            ):\n', '    max_z = np.max(z)\n', '    return z[index] - max_z - log(np.sum(np.exp(z - max_z)))\n', '\n', '\n', '\n', 'np.random.seed(1)\n', '\n', 'print(""start..."")\n', 'train(do_print_progress = True)\n', 'print(""done."")']",4.0
10,"['# We want to lot the data \n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', '\n', '# Define weights of zipfian distributuion\n', 'def zipf(index):\n', '    return 1.0 / (index + 5)\n', '\n', '# Use zipifian distribution for the classes\n', 'def zipf_sampling_weights():\n', '    return np.asarray([ zipf(i) for i in range(Param.num_classes)], dtype=np.float32)\n', '\n', 'data_sampling_distribution = lambda: zipf_sampling_weights() / np.sum(zipf_sampling_weights())\n', '\n', 'print(""start..."")\n', '\n', '\n', '# Train using uniform sampling (like before)\n', 'np.random.seed(1)\n', 'softmax_sampling_weights = lambda: np.repeat(1.0/Param.num_classes, Param.num_classes)\n', 'minibatch_data, cross_entropy_data, _ = train(do_print_progress = False)\n', '\n', '# Train using importance sampling\n', 'np.random.seed(1)\n', 'softmax_sampling_weights = zipf_sampling_weights\n', 'minibatch_data2, cross_entropy_data2, _ = train(do_print_progress = False)\n', '\n', ""plt.plot(minibatch_data, cross_entropy_data, 'r--',minibatch_data, cross_entropy_data2, 'b--')\n"", ""plt.xlabel('number of mini-batches')\n"", ""plt.ylabel('cross entropy')\n"", 'plt.show()']",5.0
13,"['print(""start..."")\n', '\n', '# Reset parameters\n', 'class Param:\n', '    # Learning parameters\n', '    learning_rate = 0.03\n', '    minibatch_size = 8\n', '    num_minbatches = 100\n', '    test_set_size = 1 # we are only interrested in speed\n', '    momentum = 0.8187307530779818\n', '    reporting_interval = 1000000 # Switch off reporting to speed up\n', '    allow_duplicates = False\n', '    \n', '    # Parameters for sampled softmax\n', '    use_sampled_softmax = True\n', '    use_sparse = True\n', '    softmax_sample_size = 10\n', '\n', '    # Details of data and model\n', '    num_classes = 50000\n', '    hidden_dim = 10\n', '    \n', 'data_sampling_distribution = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', 'softmax_sampling_weights = lambda: np.repeat(1.0 / Param.num_classes, Param.num_classes)\n', '\n', '    \n', 'sample_sizes = [5, 10, 100, 1000]\n', 'speed_with_sampled_softmax = []\n', '\n', '# Get the speed with sampled softmax for different sizes\n', 'for sample_size in sample_sizes: \n', '    print(""Measuring speed of sampled softmax for sample size %d ..."" % (sample_size))\n', '    Param.use_sampled_softmax = True\n', '    Param.softmax_sample_size = sample_size\n', '    _, _,  samples_per_second = train(do_print_progress = False)\n', '    speed_with_sampled_softmax.append(samples_per_second)\n', '\n', '# Get the speed with full softmax\n', 'Param.use_sampled_softmax = False\n', 'print(""Measuring speed of full softmax ..."")\n', '_, _,  samples_per_second = train(do_print_progress = False)\n', 'speed_without_sampled_softmax = np.repeat(samples_per_second, len(sample_sizes))\n', '\n', '# Plot the speed of sampled softmax (blue) as a function of sample sizes\n', '# and compare it to the speed with full softmax (red).    \n', ""plt.plot(sample_sizes, speed_without_sampled_softmax, 'r--',sample_sizes, speed_with_sampled_softmax, 'b--')\n"", ""plt.xlabel('softmax sample size')\n"", ""plt.ylabel('speed: instances / second')\n"", 'plt.title(""Speed \'sampled softmax\' (blue) vs. \'full softmax\' (red)"")\n', 'plt.ylim(ymin=0)\n', 'plt.show()']",6.0
14,[],
