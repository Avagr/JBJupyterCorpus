cell_id,source,language,is_reliable
0,"['# CNTK 207: Sampled Softmax\n', '\n', 'For classification and prediction problems a typical criterion function is cross-entropy with softmax. If the number of output classes is high the computation of this criterion and the corresponding gradients could be quite costly. Sampled Softmax is a heuristic to speed up training in these cases. (see: [Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model](http://www.iro.umontreal.ca/~lisa/pointeurs/importance_samplingIEEEtnn.pdf), [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410v1.pdf), [What is Candidate Sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf))\n']",English,True
1,"['**Select the notebook runtime environment devices / settings**\n', '\n', 'Before we dive into the details we run some setup that is required for automated testing of this notebook. \n']",English,True
3,"['## Basic concept\n', '\n', 'The softmax function is used in neural networks if we want to interpret the network output as a probability distribution over a set of classes $C$ with $|C|=N_C$.\n', '\n', 'Softmax maps an $N_C$-dimensional vector $z$, which has unrestricted values, to an $N_C$ dimensional vector $p$ with non-negative values that sum up to 1 so that they can be interpreted as probabilities. More precisely:\n', '\n', '$$\n', '\\begin{align}\n', 'p_i &= softmax(z, i)\\\\\n', '    &= \\frac{exp(z_i)}{\\sum_{k\\in C} exp(z_k)}\\\\\n', '\\end{align}\n', '$$\n', '\n', 'In what follows we assume that the input $z$ to the softmax is computed from some hidden vector $h$ of dimension $N_h$  in a specific way, namely:\n', '\n', '$$ z = W h + b $$\n', '\n', 'where $W$ is a learnable weight matrix of dimension $(N_c, N_h)$ and $b$ is a learnable bias vector.\n', 'We restrict ourselves to this specific choice of $z$ because it helps in implementing an efficient sampled softmax.\n', '\n', 'In a typical use-case like for example a recurrent language model, the hidden vector $h$ would be the output of the recurrent layers and $C$ would be the set of words to predict.   \n', '\n', 'As a training criterion, we use cross-entropy which is a function of the expected (true) class $t\\in C$ and the probability predicted for it:\n', '\n', '$$cross\\_entropy := -log(p_t)$$\n', '\n', '## Sampled Softmax\n', '\n', 'For the normal softmax the CNTK Python-api provides the function [cross_entropy_with_softmax](https://cntk.ai/pythondocs/cntk.ops.html?highlight=softmax#cntk.ops.cross_entropy_with_softmax). This takes as input the $N_C$-dimensional vector $z$. As mentioned for our sampled softmax implementation we assume that this z is computed by $ z = W h + b $. In sampled softmax this has to be part of the whole implementation of the criterion.\n', '\n', 'Below we show the code for `cross_entropy_with_sampled_softmax_and_embedding`. Letâ€™s look at the signature first.\n', '\n', 'One fundamental difference to the corresponding function in the Python-api (`cross_entropy_with_softmax`) is that in the Python api function the input corresponds to $z$ and must have the same dimension as the target vector, while in cross_entropy_with_full_softmax the input corresponds to our hidden vector $h$ can have any dimension (hidden_dim).\n', 'Actually, hidden_dim will be typically much lower than the dimension of the target vector.\n', '\n', 'We also have some additional parameters `num_samples, sampling_weights, allow_duplicates` that control the random sampling. \n', 'Another difference to the api function is that we return a triple (z, cross_entropy_on_samples, error_on_samples).\n', '\n', 'We will come back to the details of the implementation below.']",English,True
5,['To give a better idea of what the inputs and outputs are and how this all differs from the normal softmax we give below a corresponding function using normal softmax:'],English,True
7,"['As you can see the main differences to the api function `cross_entropy_with_softmax` are:\n', '* We include the mapping $ z = W h + b $ into the function.\n', '* We return a triple (z, cross_entropy, error_on_samples) instead of just returning the cross entropy.\n', '\n', '\n', '## A toy example\n', '\n', 'To explain how to integrate sampled softmax let us look at a toy example. In this toy example we first transform one-hot input vectors via some random projection into a lower dimensional vector $h$. The modeling task is to reverse this mapping using (sampled) softmax. Well, as already said this is a toy example.\n']",English,True
9,"['In the above code we use two different methods to report training progress:\n', '1. Using a function that computes the average cross entropy on full softmax.\n', '2. Using the built-in ProgressPrinter\n', '\n', 'ProgressPrinter reports how the value of the training criterion changes over time.\n', 'In our case the training criterion is cross-entropy from **sampled** softmax.\n', 'The same is true for the error rate computed by progress printer, this is computed only for true-class vs sampled-classes and will therefore underestimate the true error rate.\n', '\n', 'Therefore while ProgressPrinter already gives us some idea how training goes on, if we want to compare the behavior for different sampling strategies (sample size, sampling weights, ...) we should not rely on numbers that are computed only using the sampled subset of classes. \n', '\n', '\n', '## Importance sampling\n', '\n', ""Often the we don't have uniform distribution for the classes on the output side. The typical example is when we have words as output classes. A typical example are words where e.g. 'the' will be much more frequent than most others.\n"", '\n', 'In such cases one often uses a non uniform distribution for drawing the samples in sampled softmax but instead increases the sampling weight for the frequent classes. This is also called importane sampling.\n', 'In our example the sampling distribution is controlled by the weight array `softmax_sampling_weights`.\n', '\n', ""As an example let's look at the case where the classes are distrubted according to zipf-distrubtion like:\n"", '$$\n', 'p[i] \\propto \\frac{1}{i+5},\n', '$$\n', 'actually we use this distribution already in our example.\n', '\n', 'How does training behavior change if we switch uniform sampling to sampling with the zipfian distribution in sampled softmax?\n', '\n', '\n', '\n', '\n', '\n']",English,True
11,"['In the example above we compare uniform sampling (red) vs sampling with the same distribution the classes have (blue).\n', 'You will need to experiment to find the best settings for all the softmax parameters.\n']",English,True
12,"['## What speedups to expect?\n', '\n', 'The speed difference between full softmax and sampled softmax in terms of training instances depends strongly on the concrete settings, namely\n', '* Number of classes. Typically the speed-up will increase the more output classes you have.\n', '* Number of samples used in sampled softmax\n', '* Dimension of hiddlen layer input\n', '* Minibatch size\n', '* Hardware\n', '\n', 'Also you need to test how much you can reduce sample size without degradation of the result.']",English,True
